Reflex-Based Models

    Used in machine learning, deep learning, and large language models (LLMs) where inference is a fixed computation with no reasoning or backtracking.
    Applications include image recognition (CNNs), speech-to-text, object detection, and text generation (GPT, BERT, LLaMA, PaLM).
    AI systems operate using feed-forward computations, meaning they take input and produce output without considering alternative possibilities or reasoning steps.

State-Based Models

    Used in search algorithms, Markov Decision Processes (MDPs), and game-playing AI.
    Applications include *route finding (A search, Dijkstraâ€™s algorithm), reinforcement learning (Q-learning, deep Q-networks), and game AI (chess, Go, AlphaGo, self-driving cars)**.
    AI systems think in terms of states, actions, and costs to make decisions and find optimal solutions through exploration and exploitation.

Variable-Based Models

    Used in Constraint Satisfaction Problems (CSPs) and Bayesian networks to model uncertain relationships between variables.
    Common in scheduling (e.g., airline scheduling, exam timetables), tracking (e.g., Kalman filters, Hidden Markov Models), and probabilistic reasoning (e.g., medical diagnosis AI, fraud detection).
    AI models use variables and factors to represent dependencies and make inferences based on observed data.

Logic-Based Models

    Based on propositional logic, first-order logic, and knowledge representation, where AI derives conclusions using formal reasoning.
    Used in automated theorem proving, formal verification (e.g., software correctness proofs), knowledge graphs (e.g., Semantic Web, ontologies), and expert systems.
    AI systems use logical formulas and inference rules to perform symbolic reasoning and structured decision-making.
